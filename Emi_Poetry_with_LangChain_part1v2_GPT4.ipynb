{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdossou/Emi_Poetry/blob/main/Emi_Poetry_with_LangChain_part1v2_GPT4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Poetry Generator\n",
        "\n",
        "### This model is generating poetry in the style of Emily Dickinson and other poets\n"
      ],
      "metadata": {
        "id": "kEKghJQ2pmYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all relevant dependencies"
      ],
      "metadata": {
        "id": "P3WQmPI8Iotn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXsYHTgvnCM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7973d854-29be-4d9c-deb8-d64bdd16b52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI API Key"
      ],
      "metadata": {
        "id": "y4kD4Zuc1VG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up an Open-AI API Key"
      ],
      "metadata": {
        "id": "T0sLjfy8p3jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "openai.api_key = userdata.get('openai_key')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ],
      "metadata": {
        "id": "0TTosnCHnGHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions"
      ],
      "metadata": {
        "id": "15M3Jx6SBXcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def disp_markdown(text: str) -> None:\n",
        "  display(Markdown(text))"
      ],
      "metadata": {
        "id": "k3SBzWBUpQ21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  First LangChain ChatModel"
      ],
      "metadata": {
        "id": "fU4LWrv-BayH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model with Open-AI with gpt-4-turbo-preview(ChatGPT model)."
      ],
      "metadata": {
        "id": "XVkfqk4NOFWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat_model = ChatOpenAI(model_name=\"gpt-4-turbo-preview\")"
      ],
      "metadata": {
        "id": "tNscLft_nxBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a918a2-1b09-4cb9-bef8-a47d2e31fb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the roles:\n",
        "\n",
        "`role`\n",
        "- The `system` role help us \"prime\" the model to be more aligned with the task.\n",
        "\n",
        "- The `user` role.\n",
        "- The `assistant` role represents the model's output and impliments  few-shot prompt engineering.\n",
        "\n",
        "Each of these roles has a class in LangChain."
      ],
      "metadata": {
        "id": "vzGhlpwUPyU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "# The SystemMessage is associated with the system role\n",
        "system_message = SystemMessage(content=\"You are Emily Dickinson\")\n",
        "\n",
        "# The HumanMessage is associated with the user role\n",
        "user_message = HumanMessage(content=\"Write a poem about Success\")\n",
        "\n",
        "# The AIMessage is associated with the assistant role\n",
        "assistant_message = AIMessage(content=\"Success is counted sweetest By those who ne'er succeed. To comprehend a nectar Requires sorest need.\")"
      ],
      "metadata": {
        "id": "dM7lciZtoPEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing with GPT 4 and a new user message."
      ],
      "metadata": {
        "id": "dSx5HBgjSUvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_user_message = HumanMessage(content=\"Write a poem about Love\")\n",
        "\n",
        "# create the list of prompts\n",
        "list_of_prompts = [\n",
        "    system_message,\n",
        "    user_message,\n",
        "    assistant_message,\n",
        "    second_user_message\n",
        "]\n",
        "\n",
        "# Calling chat_model on the list of prompts\n",
        "chat_model(list_of_prompts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwDLOYOKSTpG",
        "outputId": "bdce6931-8689-4579-f975-82deb25c57e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Love—is anterior to Life—\\nPosterior—to Death—\\nInitial of Creation, and\\nThe Exponent of Breath—\\n\\nThis force, unseen, yet felt in depth,\\nTranscends the Earth and Skies,\\nA tether, binding heart to heart,\\nIn silent, sacred ties.\\n\\nIt whispers in the gentle wind,\\nAnd roars in stormy seas,\\nInvisible, yet palpable,\\nIn myriad forms it teases.\\n\\nLove shapes the infant's first smile,\\nAnd in the elder's last breath,\\nA continuum, unbroken,\\nFrom life unto death.\\n\\nNo measure can its depth gauge,\\nNo boundaries can confine,\\nEternal as the stars above,\\nIn every soul, it shines.\\n\\nA mystery, yet known to all,\\nWho've felt its tender grace,\\nLove—the essence of existence,\\nIn every time and place.\", response_metadata={'token_usage': {'completion_tokens': 168, 'prompt_tokens': 55, 'total_tokens': 223}, 'model_name': 'gpt-4-turbo-preview', 'system_fingerprint': 'fp_31c0f205d1', 'finish_reason': 'stop', 'logprobs': None})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This result is in line with expectations."
      ],
      "metadata": {
        "id": "pZMYJDWXTkMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates\n",
        "\n",
        "Setting up prompt templates to generate poems in multiple poets' styles on any topics."
      ],
      "metadata": {
        "id": "8DUNhabQUB8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "# setting variables for poet name and topic by wrapping in {}\n",
        "system_prompt_template = \"You are the poet {POET_NAME} and you write about {TOPIC}\"\n",
        "system_prompt_template = SystemMessagePromptTemplate.from_template(system_prompt_template)\n",
        "# this also applies to content\n",
        "user_prompt_template = \"{CONTENT}\"\n",
        "user_prompt_template = HumanMessagePromptTemplate.from_template(user_prompt_template)\n",
        "\n",
        "# put them together into a ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_prompt_template, user_prompt_template])"
      ],
      "metadata": {
        "id": "74vpojywT0-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the chat_prompt templates with an example.\n",
        "\n",
        "NOTE: disp_markdown is just a helper function to display the formatted markdown response"
      ],
      "metadata": {
        "id": "a-nbEW-kV_na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "formatted_chat_prompt = chat_prompt.format_prompt(POET_NAME=\"John Keats\", TOPIC=\"dreams\", CONTENT=\"Write a Limerick\").to_messages()\n",
        "\n",
        "disp_markdown(chat_model(formatted_chat_prompt).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "P4vd-W2FV7Xq",
        "outputId": "99a9c029-2945-4629-bcf2-f1bf26988907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In dreams, where silent whispers flow,\nA poet wandered, heart aglow.\nThrough night's soft embrace, he'd roam,\nIn visions far from earthly home,\nWhere stars in silent judgement glow."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the LangChain Chain\n",
        "\n",
        "Setting the LLMChain with the llm and the prompt then running it with an example.\n"
      ],
      "metadata": {
        "id": "hHehNFjAXbU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
        "\n",
        "disp_markdown(chain.run(POET_NAME=\"Mark Twain\", TOPIC=\"technology\", CONTENT=\"what do you think about flat water?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "lTzw4ZMoWX0X",
        "outputId": "360123fd-0df0-4b80-bf25-f71099e609d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "As Mark Twain, pondering upon the marvels and follies of mankind, I find myself bemused by the notion of \"flat water.\" In an age where the steam engine has compressed time and space, and the telegraph has made whispers travel faster than the wind, it is the simple things that still capture my fancy.\n\nFlat water, you say? A curious phrase to dwell upon. In its simplicity, it mirrors the calm expanse of the Mississippi under a moonlit sky, undisturbed but for the gentle kiss of a passing breeze. It is the canvas upon which nature paints the sunrise, a surface so serene that it becomes a mirror to the very soul of the sky.\n\nYet, in the hands of modernity, \"flat water\" has been bottled, branded, and sold back to us, commodified like the very air we breathe. We stand at the banks of progress, marveling at how we've managed to package the essence of life itself, forgetting the joy of drinking from a cool, rushing stream, untainted and free.\n\nAh, but therein lies the rub. In our quest for convenience, we've traded whispers with the river for the clinking of coins, and the melody of the natural world for the drone of machinery. \"Flat water,\" once a testament to nature's tranquility, now serves as a reminder of our disconnection from it.\n\nSo, I muse upon flat water, not for its taste or convenience, but for what it represents in the grand tapestry of human endeavor. A drop in the bucket, perhaps, but one that ripples through the fabric of our existence, questioning whether our pursuit of progress has left us parched for the very essence of life.\n\nAnd as the sun sets on another day, I cannot help but ponder: in our relentless march forward, may we have the wisdom to occasionally look back, to ensure we haven't left behind that which truly quenches the soul."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding a Document with Emily Dickenson Poems\n",
        "\n",
        "Adding a text document which contains a portion of Emily Dickenson poems."
      ],
      "metadata": {
        "id": "Md5XYaAj_t51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/financeart/EmiTalks1/raw/main/EmilyDickinsonCompleteWorks3.txt -O \"Emi_1.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4SJNvP_KXk9",
        "outputId": "e61e17a3-07c1-458a-dca7-ea3659ae62c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-20 18:24:50--  https://huggingface.co/datasets/financeart/EmiTalks1/raw/main/EmilyDickinsonCompleteWorks3.txt\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.24, 18.172.134.4, 18.172.134.88, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16815 (16K) [text/plain]\n",
            "Saving to: ‘Emi_1.txt’\n",
            "\n",
            "\rEmi_1.txt             0%[                    ]       0  --.-KB/s               \rEmi_1.txt           100%[===================>]  16.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-20 18:24:50 (98.4 MB/s) - ‘Emi_1.txt’ saved [16815/16815]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Emi_1.txt\") as f:\n",
        "    emily_dickinson = f.read()"
      ],
      "metadata": {
        "id": "HX00sL92LATv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using the CharacterTextSplitter from LangChain to split the text into chunks.\n",
        "\n",
        "Using \" \" as a separator."
      ],
      "metadata": {
        "id": "5PdfLcOlKcjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0, separator = \" \")\n",
        "texts = text_splitter.split_text(emily_dickinson)"
      ],
      "metadata": {
        "id": "BSYNeLXPKZtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9w-svpbLq62",
        "outputId": "b30f4754-a6b6-4f63-f8a6-870627af422f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using OpenAI embedings to embed the documents."
      ],
      "metadata": {
        "id": "dQCXLq-ML_aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "VigAmqxaMd5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0d390e-3dec-4849-f077-acb47985b78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing ChromaDB and tiktoken dependencies"
      ],
      "metadata": {
        "id": "uEN_IgzqOBNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb tiktoken -q"
      ],
      "metadata": {
        "id": "Y-ZuzHPCOjLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6c2c78-a5e0-4a41-b0dd-0bde67286c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()"
      ],
      "metadata": {
        "id": "ql7jqj7TONDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the embeded documents with a query using natural language."
      ],
      "metadata": {
        "id": "kfn0R64lPb7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain the main topic in the poem ROUGE ET NOIR?\"\n",
        "docs = docsearch.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "ubZwxCHvQzsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4M08F78Q3i3",
        "outputId": "7947efbb-313d-407d-a4ca-45e98655b124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"more.\\n\\n\\nUnmoved, she notes the chariot's pausing\\nAt her low gate;\\n\\nUnmoved, an emperor is kneeling\\nUpon her mat.\\n\\n\\nI've known her from an ample nation\\nChoose one;\\n\\nThen close the valves of her attention\\nLike stone.\\n\\nTHE SECRET.\\n\\n\\n\\nSome things that fly there be, --\\nBirds, hours, the bumble-bee:\\nOf these no elegy.\\n\\n\\n\\nSome things that stay there be, --\\nGrief, hills, eternity:\\nNor this behooveth me.\\n\\n\\nThere are, that resting, rise.\\nCan I expound the skies?\\nHow still the riddle lies!\\n\\n\\n\\n\\nTHE LONELY\", metadata={'source': '9'})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the chain using `load_qa_chain` to integrate the documents with OpenAI LLM using the`stuff` chain type."
      ],
      "metadata": {
        "id": "-8W9ZmNaRRBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = load_qa_chain(llm=chat_model, chain_type=\"stuff\")\n",
        "query = \"Explain the main topic in the poem ROUGE ET NOIR\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "S7vAWKiFSVj_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "066fd96c-1d48-4a23-baf4-da67712e5a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The poem \"Rouge et Noir\" delves into themes of risk, chance, and the dichotomy between loss and victory as metaphorically represented through the imagery of gambling. \"Rouge et Noir,\" translating from French to \"Red and Black,\" refers to a gamble or a choice between two outcomes, much like the betting that occurs in the roulette game of the same name. \\n\\nIn this poem, the soul is personified and faces a decision akin to placing a wager. This gamble is not about money but about existential stakes, suggesting that life itself is a series of gambles where the choices we make lead to outcomes of loss or gain. The mention of \"Hundreds have lost, indeed, / But tens have won an all\" reflects the harsh reality that while many may not succeed in their endeavors (or the gambles they take in life), a few will achieve everything they desire or strive for. \\n\\nThe poem further anthropomorphizes angels and imps, indicating a celestial observation or interest in the soul\\'s gamble, adding a spiritual dimension to the decisions and risks one takes. The angels and imps seem to be waiting for the outcome of the soul\\'s decision, suggesting that these choices have implications beyond the mortal realm, possibly touching on the fate of the soul in the afterlife.\\n\\nIn essence, \"Rouge et Noir\" explores the inherent risks in the choices and challenges of life, the uncertainty of outcomes, and the eternal consequences of our earthly gambles.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is adapted from the notebook developed by AI Makerspace, which was originally using Alice in Wonderland text."
      ],
      "metadata": {
        "id": "8LdfhFT0MxhB"
      }
    }
  ]
}